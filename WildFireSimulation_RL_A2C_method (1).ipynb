{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzZatzmPZlCo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random, math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "class UnionFind:\n",
        "    \"\"\"简单的并查集，用于判断节点连通情况\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.parent = list(range(n))\n",
        "\n",
        "    def find(self, x):\n",
        "        while self.parent[x] != x:\n",
        "            self.parent[x] = self.parent[self.parent[x]]\n",
        "            x = self.parent[x]\n",
        "        return x\n",
        "\n",
        "    def union(self, x, y):\n",
        "        rx = self.find(x)\n",
        "        ry = self.find(y)\n",
        "        if rx != ry:\n",
        "            self.parent[ry] = rx\n",
        "\n",
        "class Region:\n",
        "    def __init__(self, id, pos, H, region_discount_rate, ARC):\n",
        "        \"\"\"\n",
        "        参数：\n",
        "          id: 区域编号\n",
        "          pos: 区域在二维平面上的位置 (x, y)\n",
        "          H: 燃料量（血量），取值 100～1000\n",
        "          region_discount_rate: 区域自然衰减的折扣率（0～1）\n",
        "          ARC: 面积（公顷）\n",
        "        \"\"\"\n",
        "        self.id = id\n",
        "        self.pos = np.array(pos)\n",
        "        self.H = H\n",
        "        self.max_health = H\n",
        "        self.region_discount_rate = region_discount_rate\n",
        "        self.ARC = ARC\n",
        "        self.K = 1.0  # 火焰强度初始为 0\n",
        "        # 定义燃烧系数，设定常数 c（这里取 0.001），使得燃烧系数 = c * ARC\n",
        "        self.transmisive = 5\n",
        "\n",
        "    @property\n",
        "    def active(self):\n",
        "        \"\"\"如果区域尚未烧尽，则返回 1，否则 0\"\"\"\n",
        "        return 1 if self.H > 0 else 0\n",
        "\n",
        "    def pos_vector(self):\n",
        "        #return a standarized position vector\n",
        "        return 0\n",
        "\n",
        "class Edge:\n",
        "    def __init__(self, region1, region2, border_direction, shared_border_area):\n",
        "        \"\"\"\n",
        "        表示两个区域之间的接壤关系\n",
        "        参数：\n",
        "          region1, region2: 相邻的两个 Region 对象\n",
        "          border_direction: 接壤朝向（一个二维单位向量），这里约定为从 region1 指向 region2\n",
        "          shared_border_area: 接壤面积，取两区域面积平方根的较小值\n",
        "        \"\"\"\n",
        "        self.region1 = region1\n",
        "        self.region2 = region2\n",
        "        self.border_direction = border_direction  # np.array，形状 (2,)\n",
        "        self.shared_border_area = shared_border_area\n",
        "\n",
        "class WildfireSimulation:\n",
        "    def __init__(self, num_regions):\n",
        "        self.num_regions = num_regions\n",
        "        self.nodes = {}      # {region id: Region}\n",
        "        self.edges = []      # Edge 列表\n",
        "        self.adjacency = {}  # 邻接表：{region id: [(neighbor_id, Edge), ...]}\n",
        "        self.history = []    # 记录每个时间步的状态: (t, env_factors, state)\n",
        "        self.gamma = 0.8     # 折扣率（用于火焰传播中描述自身衰减）\n",
        "\n",
        "        self.init_regions()\n",
        "        self.init_edges()      # 保证生成的 graph 是 fully connected 的\n",
        "        self.init_adjacency()\n",
        "\n",
        "        # 指定火源（起火源）节点，默认为节点 0，并预先点燃\n",
        "        # self.fire_origin = np.random.randint(0, self.num_regions)\n",
        "        self.fire_origin = np.random.randint(0, 3)\n",
        "        self.nodes[self.fire_origin].K = 50.0\n",
        "\n",
        "    def init_regions(self):\n",
        "        \"\"\"随机生成区域及其属性\"\"\"\n",
        "        for i in range(self.num_regions):\n",
        "            pos = (random.uniform(0, 100), random.uniform(0, 100))\n",
        "            # H = random.uniform(600, 1000)\n",
        "            H = 1000\n",
        "            region_discount_rate = 0.2\n",
        "            ARC = 10\n",
        "            self.nodes[i] = Region(i, pos, H, region_discount_rate, ARC)\n",
        "\n",
        "    def init_edges(self):\n",
        "        \"\"\"\n",
        "        根据区域在平面上的位置，若两区域中心距离小于阈值，则认为它们相邻，建立边。\n",
        "        为了保证 graph 是 fully connected 的，我们先按阈值生成边，\n",
        "        然后利用并查集判断连通性，若存在不连通部分，再从所有可能的节点对中按距离最小的顺序依次添加边，\n",
        "        直到整个 graph 连通。\n",
        "        \"\"\"\n",
        "        self.edges = []\n",
        "        node_ids = list(self.nodes.keys())\n",
        "        threshold = 30.0  # 邻近判断阈值\n",
        "\n",
        "        # 根据阈值生成初始的边\n",
        "        for i in range(len(node_ids)):\n",
        "            for j in range(i+1, len(node_ids)):\n",
        "                node_i = self.nodes[node_ids[i]]\n",
        "                node_j = self.nodes[node_ids[j]]\n",
        "                dist = np.linalg.norm(node_i.pos - node_j.pos)\n",
        "                if dist < threshold:\n",
        "                    direction = node_j.pos - node_i.pos\n",
        "                    if np.linalg.norm(direction) != 0:\n",
        "                        border_direction = direction / np.linalg.norm(direction)\n",
        "                    else:\n",
        "                        border_direction = np.array([1.0, 0.0])\n",
        "                    shared_border_area = min(math.sqrt(node_i.ARC), math.sqrt(node_j.ARC))\n",
        "                    self.edges.append(Edge(node_i, node_j, border_direction, shared_border_area))\n",
        "\n",
        "        # 利用并查集检查当前 graph 的连通性\n",
        "        uf = UnionFind(self.num_regions)\n",
        "        for edge in self.edges:\n",
        "            uf.union(edge.region1.id, edge.region2.id)\n",
        "\n",
        "        # 将所有节点对按距离从小到大排序\n",
        "        all_pairs = []\n",
        "        for i in range(len(node_ids)):\n",
        "            for j in range(i+1, len(node_ids)):\n",
        "                node_i = self.nodes[node_ids[i]]\n",
        "                node_j = self.nodes[node_ids[j]]\n",
        "                dist = np.linalg.norm(node_i.pos - node_j.pos)\n",
        "                all_pairs.append((dist, node_ids[i], node_ids[j]))\n",
        "        all_pairs.sort(key=lambda x: x[0])\n",
        "\n",
        "        # 如果有不连通的部分，依次添加最小距离边，直到所有节点都连通\n",
        "        for dist, i, j in all_pairs:\n",
        "            if uf.find(i) != uf.find(j):\n",
        "                node_i = self.nodes[i]\n",
        "                node_j = self.nodes[j]\n",
        "                direction = node_j.pos - node_i.pos\n",
        "                if np.linalg.norm(direction) != 0:\n",
        "                    border_direction = direction / np.linalg.norm(direction)\n",
        "                else:\n",
        "                    border_direction = np.array([1.0, 0.0])\n",
        "                shared_border_area = min(math.sqrt(node_i.ARC), math.sqrt(node_j.ARC))\n",
        "                self.edges.append(Edge(node_i, node_j, border_direction, shared_border_area))\n",
        "                uf.union(i, j)\n",
        "\n",
        "    def init_adjacency(self):\n",
        "        \"\"\"建立邻接表，每个区域记录所有相邻区域及对应边信息\"\"\"\n",
        "        for node_id in self.nodes:\n",
        "            self.adjacency[node_id] = []\n",
        "        for edge in self.edges:\n",
        "            id1 = edge.region1.id\n",
        "            id2 = edge.region2.id\n",
        "            self.adjacency[id1].append((id2, edge))\n",
        "            self.adjacency[id2].append((id1, edge))\n",
        "\n",
        "    def run_session_TD(self, max_iter):\n",
        "        \"\"\"\n",
        "        外部控制方法：运行 T 个时间步的模拟。\n",
        "        首先记录初始状态（t=0），然后逐步调用 next_step 来推进模拟。\n",
        "        \"\"\"\n",
        "        # 记录初始状态（t=0）\n",
        "        initial_env = {\n",
        "            'wind_direction': None,\n",
        "            'wind_strength': None,\n",
        "            'dryness': None,\n",
        "            'temperature': None\n",
        "        }\n",
        "        state = {node_id: {'pos': node.pos.copy(), 'H': node.H, 'K': node.K}\n",
        "                 for node_id, node in self.nodes.items()}\n",
        "        self.history.append((0, initial_env, state, None))\n",
        "\n",
        "        # 逐步模拟，从 t=1 到 end\n",
        "        t = 0\n",
        "        while t < max_iter:\n",
        "            t += 1\n",
        "            self.next(t)\n",
        "            if self.is_end():\n",
        "                print(f\"game end at {t}\")\n",
        "                self.history.append('end')\n",
        "                break\n",
        "        self.history.append('end')\n",
        "\n",
        "    def is_end(self):\n",
        "        contained = True\n",
        "        for node in self.nodes.keys():\n",
        "            if self.nodes[node].H >= 1 and self.nodes[node].K >= 1:\n",
        "                contained = False\n",
        "        return contained\n",
        "\n",
        "    def next(self, t, agent):\n",
        "        \"\"\"\n",
        "        执行从时刻 t-1 到时刻 t 的一步模拟：\n",
        "         - 生成当前的环境因子（风向、风力、干燥度、温度）\n",
        "         - 计算每个区域的火焰传播：计算 retention, incoming, outgoing，\n",
        "           得到新的火焰强度 new_K\n",
        "         - 根据 new_K 更新每个区域的火焰强度 K 和剩余燃料 H（考虑死亡率）\n",
        "         - 将当前时刻的状态记录到 history 中\n",
        "        \"\"\"\n",
        "        # print(f\"--------------------------------{t}---------------------------\")\n",
        "        # 生成环境因子\n",
        "        wind_angle = random.uniform(0, 2*math.pi)\n",
        "        # wind_angle = 1\n",
        "        wind_direction = np.array([math.cos(wind_angle), math.sin(wind_angle)])\n",
        "        # wind_strength = random.uniform(0, 1)\n",
        "        wind_strength = 1\n",
        "        dryness = 1.5\n",
        "        # temperature = random.uniform(0, 1)\n",
        "        temperature = 1\n",
        "        env_factors = {\n",
        "            'wind_direction': wind_direction,\n",
        "            'wind_strength': wind_strength,\n",
        "            'dryness': dryness,\n",
        "            'temperature': temperature\n",
        "        }\n",
        "\n",
        "        # 这里定义一个因子，用于U->V之间的传播计算\n",
        "        factor = 0.5 * (dryness + temperature)\n",
        "\n",
        "        new_K = {}\n",
        "        # 对每个区域计算火焰强度的更新\n",
        "        for node_id, node in self.nodes.items():\n",
        "            if node.H < 0:\n",
        "                # print(f\"node {node_id} stop transimission\")\n",
        "                new_K[node_id] = 0.0\n",
        "                node.H = 0\n",
        "            else:\n",
        "                # retention 部分（内部传播保持部分）\n",
        "                retention = node.active * node.transmisive * node.K\n",
        "                incoming = 0.0\n",
        "                outgoing = 0.0\n",
        "\n",
        "                # 遍历邻接区域，计算火焰传播（incoming 和 outgoing）\n",
        "                for (nbr_id, edge) in self.adjacency[node_id]:\n",
        "                    neighbor = self.nodes[nbr_id]\n",
        "                    if edge.region1.id == node_id:\n",
        "                        phi_in = np.dot(wind_direction, -edge.border_direction) * edge.shared_border_area * wind_strength\n",
        "                    else:\n",
        "                        phi_in = np.dot(wind_direction, edge.border_direction) * edge.shared_border_area * wind_strength\n",
        "                    incoming += factor * neighbor.K * phi_in\n",
        "\n",
        "                    if edge.region1.id == node_id:\n",
        "                        phi_out = np.dot(wind_direction, edge.border_direction) * edge.shared_border_area * wind_strength\n",
        "                    else:\n",
        "                        phi_out = np.dot(wind_direction, -edge.border_direction) * edge.shared_border_area * wind_strength\n",
        "                    outgoing += factor * node.K * phi_out\n",
        "\n",
        "                # 计算新火焰强度\n",
        "                K_new_value = retention + max(0, incoming) - max(0, outgoing)\n",
        "                new_K[node_id] = K_new_value if K_new_value > 0 else 0.0\n",
        "\n",
        "                # print(f\"node {node_id}\")\n",
        "                # print(f\"K new is: {retention} + {incoming} - {outgoing}\")\n",
        "                # print(f\" node {node.id} K: {node.K} -> {K_new_value}\")\n",
        "\n",
        "        # 更新所有节点的火焰强度 K 和剩余燃料 H（这里根据传播模型计算死亡量）\n",
        "        # print(\"在行动之前的新 K 值：\", list(new_K.values()))\n",
        "        #action的状态转移，影响的是newK\n",
        "        action, dist = agent.action(self)\n",
        "        action = action.detach().cpu().numpy()\n",
        "\n",
        "        new_K = self.apply_action(action, agent, new_K)\n",
        "        # print(\"行动之后的新 K 值：\", list(new_K.values()))\n",
        "        #转移之后，根据最后的newK来更新\n",
        "        for node_id, node in self.nodes.items():\n",
        "            node.K = new_K[node_id]\n",
        "            if node.H > 0:\n",
        "                # new_death 表示在本次更新中因火焰传播造成的死亡\n",
        "                new_death = node.region_discount_rate * new_K[node_id]\n",
        "                # 此处更新：先扣除死亡部分，再更新剩余 H\n",
        "                new_K[node_id] -= new_death\n",
        "                node.H -= new_death\n",
        "\n",
        "            if node.H<0:\n",
        "                node.H = 0\n",
        "                node.K = 0\n",
        "\n",
        "\n",
        "        # 记录当前时刻状态\n",
        "        state = {node_id: {'pos': node.pos.copy(), 'H': node.H, 'K': node.K}\n",
        "                 for node_id, node in self.nodes.items()}\n",
        "        self.history.append((t, env_factors, state, action))\n",
        "\n",
        "\n",
        "    def apply_action(self, action, agent, newK):\n",
        "        \"\"\"\n",
        "        状态转移函数 P(s_t' | s_t, a_t)：\n",
        "\n",
        "        agent 的救火 action 会对环境 V_t 中的每个节点产生抑制作用。具体规则如下：\n",
        "          对于每一个节点 i，其火焰强度更新为：\n",
        "\n",
        "              i.K = max(0, i.K - (N * 100))\n",
        "\n",
        "          其中，N 表示分配给该区域的飞机数量，计算公式：\n",
        "\n",
        "              N = agent.J * weight_i\n",
        "\n",
        "          其中 action 是一个权重向量 a_t = [weight_0, weight_1, ..., weight_{n-1}]\n",
        "          满足 ∑_i weight_i = 1；agent.J 为消防飞机总数。\n",
        "\n",
        "        参数:\n",
        "          action: 列表或数组，长度等于节点数，每一项为对应节点分配的飞机百分比（权重）。\n",
        "          agent: FirefightingAgent 对象，其属性 J 表示消防飞机总数。\n",
        "        \"\"\"\n",
        "        # 遍历所有节点，根据对应的 action 权重更新火焰强度 K\n",
        "        final_K = {}\n",
        "        for i in range(0, len(list(newK.keys()))):\n",
        "            weight_i = action[i]\n",
        "            # 计算分配到该节点的飞机数量\n",
        "            N = agent.J * weight_i\n",
        "            reduction = N * agent.capability  # 每架飞机减少 100 单位火焰强度\n",
        "            # 更新火焰强度，并确保不为负数\n",
        "            K_before = newK[i]\n",
        "            K_after = max(0, K_before - reduction)\n",
        "            final_K[i] = K_after\n",
        "            # print(f\"节点 {i}: K {K_before:.1f} -> {K_after:.1f}，飞机分配: {N:.2f} (weight {weight_i:.2f})\")\n",
        "\n",
        "        return final_K #字典final_K\n",
        "\n",
        "\n",
        "    def return_lists(self):\n",
        "        #at time t: (t, env_factors (Bt), state (Vt)), Vt state: {node_id: {pos, H, K} for each node_id}\n",
        "        E = self.adjacency\n",
        "        max_health_total = np.sum([self.nodes[id].max_health for id in range(0,self.num_regions)])\n",
        "        print(self.history)\n",
        "        B_list = []\n",
        "        Vt_list = []\n",
        "        a_list = []\n",
        "        rt_list = []\n",
        "\n",
        "        t = 1\n",
        "        while self.history[t+1] != 'end':\n",
        "            Bt = self.history[t][1]\n",
        "            Vt = self.history[t][2]\n",
        "            Vnext = self.history[t+1][2]\n",
        "            at = self.history[t][3]\n",
        "\n",
        "            H_total_t = np.sum([list(Vt.values())[i]['H'] for i in range(0, self.num_regions)])\n",
        "            H_total_next = np.sum([list(Vnext.values())[i]['H'] for i in range(0, self.num_regions)])\n",
        "            diff = H_total_next - H_total_t\n",
        "            rt = diff - 10\n",
        "            B_list.append(Bt)\n",
        "            Vt_list.append(Vt)\n",
        "            rt_list.append(rt)\n",
        "            a_list.append(at)\n",
        "            t+=1\n",
        "\n",
        "        #get reward for last session\n",
        "        Bt = self.history[t][1]\n",
        "        Vt = self.history[t][2]\n",
        "        at = self.history[t][3]\n",
        "        Ht_values = [list(Vt.values())[node]['H'] for node in range(0, self.num_regions)]\n",
        "        Ht_max_values = [self.nodes[node].max_health for node in range(0, self.num_regions)]\n",
        "        num_protected = np.sum([1 for node in range(0, self.num_regions) if (Ht_max_values[node]-Ht_values[node])/Ht_max_values[node] <= 0.10 ])\n",
        "        print(f\"num protected: {num_protected}\")\n",
        "        rt = np.sum(Ht_values) + num_protected*500\n",
        "        rt_list.append(rt)\n",
        "        Vt_list.append(Vt)\n",
        "        B_list.append(Bt)\n",
        "        a_list.append(at)\n",
        "        assert len(rt_list) == t\n",
        "        return (E, Vt_list, B_list, a_list, rt_list)\n",
        "\n",
        "\n",
        "    def plot_at_t(self, t):\n",
        "        \"\"\"\n",
        "        绘制指定时刻 t 的区域状态：\n",
        "        - 每个区域根据其在二维平面上的位置绘制，并用颜色显示剩余燃料 H\n",
        "        - 图中标题中显示当前环境因子信息（如果有），否则显示“初始状态”\n",
        "        \"\"\"\n",
        "        record = None\n",
        "        for rec in self.history:\n",
        "            if rec[0] == t:\n",
        "                record = rec\n",
        "                break\n",
        "        if record is None:\n",
        "            print(\"未找到时刻 {} 的记录。\".format(t))\n",
        "            return\n",
        "        time_step, env_factors, state, action = record\n",
        "\n",
        "        xs, ys, colors, labels = [], [], [], []\n",
        "        for node_id, info in state.items():\n",
        "            xs.append(info['pos'][0])\n",
        "            ys.append(info['pos'][1])\n",
        "            colors.append(info['H'])\n",
        "            labels.append(f\"{node_id}\\nH:{info['H']:.1f}\\nK:{info['K']:.1f}\")\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sc = plt.scatter(xs, ys, c=colors, cmap='hot', s=200, vmin=0, vmax=1000)\n",
        "        for i, txt in enumerate(labels):\n",
        "            plt.annotate(txt, (xs[i], ys[i]), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
        "        plt.colorbar(sc, label=\"remained H\")\n",
        "\n",
        "        # 绘制连通关系（灰色虚线）\n",
        "        for edge in self.edges:\n",
        "            pos1 = edge.region1.pos\n",
        "            pos2 = edge.region2.pos\n",
        "            plt.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]],\n",
        "                    color='gray', linestyle='--', linewidth=1, zorder=0)\n",
        "\n",
        "        # 如果环境因子中包含风向信息，则绘制风向箭头\n",
        "        if env_factors['wind_direction'] is not None:\n",
        "            wind_direction = env_factors['wind_direction']\n",
        "            arrow_scale = 15  # 箭头长度的缩放因子，可根据需要调整\n",
        "            # 固定绘制位置，例如 (50,50)\n",
        "            x0 = 50\n",
        "            y0 = 50\n",
        "            head_width = 10 * env_factors['wind_strength'] if env_factors['wind_strength'] is not None else 0\n",
        "            plt.arrow(x0, y0,\n",
        "                    wind_direction[0] * arrow_scale,\n",
        "                    wind_direction[1] * arrow_scale,\n",
        "                    head_width=head_width, head_length=4, fc='blue', ec='blue', linewidth=2)\n",
        "            plt.text(x0, y0, \"Wind\", color='blue', fontsize=12)\n",
        "            title_str = (\"time {} s firesimul\\n: w_direct {} | w_velocity: {:.2f} | dryness: {} | temp: {:.2f}\"\n",
        "                        .format(time_step,\n",
        "                                np.round(env_factors['wind_direction'], 2),\n",
        "                                env_factors['wind_strength'],\n",
        "                                env_factors['dryness'],\n",
        "                                env_factors['temperature']))\n",
        "        else:\n",
        "            # t=0 时未设置环境因子\n",
        "            title_str = f\"time {time_step} s firesimul original state\"\n",
        "\n",
        "        plt.title(title_str)\n",
        "        plt.xlabel(\"X 坐标\")\n",
        "        plt.ylabel(\"Y 坐标\")\n",
        "        plt.xlim([-5, 105])\n",
        "        plt.ylim([-5, 105])\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 定义一个初始化函数\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):  # 只对Linear层进行初始化\n",
        "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "def check_parameters(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if torch.isnan(param).any():\n",
        "                print(f\"⚠️ 参数 {name} 出现 NaN 值！\")\n",
        "            if torch.isinf(param).any():\n",
        "                print(f\"⚠️ 参数 {name} 出现 Inf 值！\")\n",
        "def check_gradients(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.grad is not None:\n",
        "                if torch.isnan(param.grad).any():\n",
        "                    print(f\"⚠️ 梯度 {name} 出现 NaN 值！\")\n",
        "                if torch.isinf(param.grad).any():\n",
        "                    print(f\"⚠️ 梯度 {name} 出现 Inf 值！\")\n",
        "            else:\n",
        "                print(f\"⚠️ 梯度 {name} 为 None，可能没有参与计算图！\")\n",
        "def print_model_parameters(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"参数名: {name}\")\n",
        "        print(f\"参数值: {param.data}\")  # 打印参数值\n",
        "        if torch.isnan(param).any():\n",
        "            print(f\"⚠️ 参数 {name} 出现 NaN 值！\")\n",
        "        if torch.isinf(param).any():\n",
        "            print(f\"⚠️ 参数 {name} 出现 Inf 值！\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "\n",
        "class A2CNetwork(nn.Module):\n",
        "    def __init__(self, num_nodes, env_dim=5, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        num_nodes: 节点个数\n",
        "        env_dim: 环境向量的维度（例如 [wind_direction_x, wind_direction_y, wind_strength, dryness, temperature]）\n",
        "        hidden_dim: 隐藏层维度\n",
        "        \"\"\"\n",
        "        super(A2CNetwork, self).__init__()\n",
        "        # 对环境信息做全连接嵌入\n",
        "        self.env_fc = nn.Linear(env_dim, hidden_dim)\n",
        "        # 对图节点的不变特征（X_invariant，形状: (num_nodes,2)）与变化特征（X_variant，形状: (num_nodes,2)）进行拼接后为4维，\n",
        "        # 然后映射到 hidden_dim 维\n",
        "        self.gcn_fc = nn.Linear(4, hidden_dim)\n",
        "        self.gcn_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # 最后将所有节点的隐藏表示汇聚（这里采用求和）得到图嵌入\n",
        "        # 接着将图嵌入与环境嵌入拼接后输入 MLP\n",
        "        self.actor_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim*2),\n",
        "            nn.LayerNorm(hidden_dim*2),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, num_nodes)     # 输出 logits（未归一化）——对应每个节点的分配权重\n",
        "        )\n",
        "        self.critic_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),  # 添加 LayerNorm\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.LayerNorm(32),  # 添加 LayerNorm\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        # 使用 apply() 方法遍历并应用初始化\n",
        "        self.env_fc.apply(init_weights)\n",
        "        self.gcn_fc.apply(init_weights)\n",
        "        self.gcn_fc2.apply(init_weights)\n",
        "        self.actor_mlp.apply(init_weights)\n",
        "        self.critic_mlp.apply(init_weights)\n",
        "\n",
        "\n",
        "    def forward(self, env_vector, X_invariant, X_variant, A):\n",
        "        \"\"\"\n",
        "        参数：\n",
        "          env_vector: Tensor, shape (env_dim,) ；当前环境外部信息\n",
        "          X_invariant: Tensor, shape (num_nodes, 2)；节点的不变特征（例如位置）\n",
        "          X_variant: Tensor, shape (num_nodes, 2)；节点的变化特征（例如当前 K, H）\n",
        "          A: Tensor, shape (num_nodes, num_nodes)；邻接矩阵（可加上自环），这里采用简单的加和聚合\n",
        "        返回：\n",
        "          action_probs: Tensor, shape (num_nodes,)，各节点权重分布（softmax 后）\n",
        "          value: Tensor, shape (1,) 状态价值估计\n",
        "        \"\"\"\n",
        "        # 拼接节点特征：得到 (num_nodes, 4)\n",
        "        # print(f\"第一层X embedding gcn_fc的参数:{print_model_parameters(self.gcn_fc)}\")\n",
        "\n",
        "        X = torch.cat([X_invariant, X_variant], dim=1)\n",
        "        X = (X - X.mean(dim=0)) / (X.std(dim=0) + 1e-8)\n",
        "        # print(f\"X = [X_invariant, X_variant]: {X}\")\n",
        "        # print(f\"A : {A}\")\n",
        "        h = F.relu(self.gcn_fc(X))  # (num_nodes, hidden_dim)\n",
        "\n",
        "        # 简单的GCN聚合：h = A * h （邻接矩阵乘法，假设 A 已构造好）\n",
        "        h = torch.matmul(A, h)\n",
        "        # print(f\"h=matmul(A, h)时候：{h}\")\n",
        "        h = F.relu(self.gcn_fc2(h))  # (num_nodes, hidden_dim)\n",
        "        # print(f\"h=F_relu(gcn_fc2(h))时候：{h}\")\n",
        "\n",
        "        # 聚合所有节点的表示，得到图嵌入：采用求和（也可以采用平均）\n",
        "        graph_emb = torch.sum(h, dim=0)  # (hidden_dim,)\n",
        "        # 环境信息嵌入\n",
        "        env_emb = F.relu(self.env_fc(env_vector))  # (hidden_dim,)\n",
        "        # 拼接图嵌入和环境嵌入\n",
        "        combined = torch.cat([graph_emb, env_emb], dim=0)  # (hidden_dim*2,)\n",
        "        if torch.isnan(combined).any():\n",
        "          print(\"⚠️ 输入 combined 出现 NaN 值！\")\n",
        "          print(f\"combined is {combined}\")\n",
        "          print(f\"env_emb is {env_emb}\")\n",
        "          print(f\"graph_emb is {graph_emb}\")\n",
        "        if torch.isinf(combined).any():\n",
        "          print(\"⚠️ 输入 combined 出现 Inf 值！\")\n",
        "\n",
        "        # Actor: 输出各节点的 logits，经过 softmax 得到概率分布\n",
        "        logits = self.actor_mlp(combined)  # (num_nodes,)\n",
        "        # print(\"--------------->>>>>>>>>>>>>\")\n",
        "        # print(f\"原生的logits in actor_mlp is {logits}\")\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"⚠️ logits 出现 NaN 值！\")\n",
        "            raise ValueError(\"logits 出现 NaN 值！\")\n",
        "        if torch.isinf(logits).any():\n",
        "            print(\"⚠️ logits 出现 Inf 值！\")\n",
        "        # print(\"---------------<<<<<<<<<<<<<\")\n",
        "        logits = logits - logits.max(dim=-1, keepdim=True)[0]\n",
        "        action_probs = F.softmax(logits, dim=-1)\n",
        "        if torch.isnan(action_probs).any() or torch.isinf(action_probs).any():\n",
        "          print(\"⚠️ 策略分布出现异常！\")\n",
        "        # Critic: 状态价值\n",
        "        value = self.critic_mlp(combined)  # (1,)\n",
        "        return action_probs, value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, num_nodes, env_dim=5, hidden_dim=256, lr=0.001):\n",
        "        # 消防飞机资源\n",
        "        self.J = 20\n",
        "        self.capability = 100  # 每架飞机对应减少火焰强度的单位（例如 100）\n",
        "        # 建立 A2C 网络，输入维度由 num_nodes 和 env_dim 决定\n",
        "        self.network = A2CNetwork(num_nodes, env_dim, hidden_dim)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "\n",
        "        self.gamma = 0.9  # 折扣因子\n",
        "        self.dirichlet_concentration = 10.0 #用于构造 Dirichlet 分布\n",
        "\n",
        "        self.loss_history = []\n",
        "\n",
        "    def get_state(self, simulation):\n",
        "        \"\"\"\n",
        "        从 simulation 中提取 RL 使用的状态：\n",
        "          - env_vector: 由 env_factors 构造，形如 [wind_direction_x, wind_direction_y, wind_strength, dryness, temperature]\n",
        "            如果 env_factors 为 None（如 t=0）则置零。\n",
        "          - X_invariant: 每个节点的固定特征，这里取节点位置 (x, y)；形状 (num_nodes, 2)\n",
        "          - X_variant: 每个节点的动态特征，这里取 (K, H)；形状 (num_nodes, 2)\n",
        "          - A: 邻接矩阵（FloatTensor），尺寸 (num_nodes, num_nodes)。这里简单构造二值矩阵并加上单位对角线。\n",
        "        \"\"\"\n",
        "        # 取 simulation.history 中最后一条记录（若最后记录为 'end' 则取倒数第二条）\n",
        "        if simulation.history[-1] == 'end':\n",
        "            t, env_factors, state, _ = simulation.history[-2]\n",
        "        else:\n",
        "            t, env_factors, state, _ = simulation.history[-1]\n",
        "        num_nodes = simulation.num_regions\n",
        "\n",
        "        # env_vector: 如果 env_factors 为 None，则置零\n",
        "        if env_factors['wind_direction'] is not None:\n",
        "            wind_dir = env_factors['wind_direction']\n",
        "            wind_strength = env_factors['wind_strength']\n",
        "            dryness = env_factors['dryness']\n",
        "            temperature = env_factors['temperature']\n",
        "            env_vector = torch.tensor([wind_dir[0], wind_dir[1], wind_strength, dryness, temperature], dtype=torch.float)\n",
        "        else:\n",
        "            env_vector = torch.zeros(5, dtype=torch.float)\n",
        "\n",
        "        # X_invariant: 取每个节点的 pos (x, y)\n",
        "        X_invariant = []\n",
        "        # X_variant: 取每个节点的 [K, H]\n",
        "        X_variant = []\n",
        "        for i in range(num_nodes):\n",
        "            node_state = state[i]\n",
        "            pos = node_state['pos']  # 假设 pos 是 numpy 数组或列表\n",
        "            X_invariant.append([pos[0], pos[1]])\n",
        "            X_variant.append([node_state['K'], node_state['H']])\n",
        "        X_invariant = torch.tensor(X_invariant, dtype=torch.float)  # shape (num_nodes, 2)\n",
        "        X_variant = torch.tensor(X_variant, dtype=torch.float)      # shape (num_nodes, 2)\n",
        "\n",
        "        # 构造邻接矩阵 A：根据 simulation.adjacency，令 A[i,j]=1 如果存在边，否则 0，然后加上自环\n",
        "        A = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
        "        for i in range(num_nodes):\n",
        "            for (j, edge) in simulation.adjacency[i]:\n",
        "                A[i, j] = 1.0\n",
        "        # 加自环\n",
        "        A += torch.eye(num_nodes)\n",
        "        # 这里可以考虑归一化 A（例如用对称归一化），此处简单使用\n",
        "        return env_vector, X_invariant, X_variant, A\n",
        "\n",
        "    # def action(self, simulation):\n",
        "    #     \"\"\"\n",
        "    #     从 simulation 提取状态，并利用当前网络计算动作概率分布，\n",
        "    #     然后返回一个 numpy 数组表示各节点的飞机分配权重。\n",
        "    #     \"\"\"\n",
        "    #     env_vector, X_invariant, X_variant, A = self.get_state(simulation)\n",
        "    #     action_probs, _ = self.network(env_vector, X_invariant, X_variant, A)\n",
        "    #     # action_probs 是一个 tensor（num_nodes,），转换为 numpy 数组\n",
        "    #     return action_probs.detach().cpu().numpy()\n",
        "\n",
        "    def action(self, simulation):\n",
        "        \"\"\"\n",
        "        从 simulation 提取状态，并利用当前网络计算动作分布（一个概率向量）。\n",
        "        这里我们采用 Dirichlet 分布，将网络输出的 softmax 概率乘以一个浓度参数，\n",
        "        得到一个 Dirichlet 分布，从中采样动作。\n",
        "        \"\"\"\n",
        "        env_vector, X_invariant, X_variant, A = self.get_state(simulation)\n",
        "        # 网络输出 actor_logits，然后通过 softmax 得到概率向量\n",
        "        # 假设网络内部已输出 action_probs（即 softmax(logits)）\n",
        "        action_probs, _ = self.network(env_vector, X_invariant, X_variant, A)\n",
        "        # 为避免数值问题，确保 action_probs > 0\n",
        "        action_probs = torch.nan_to_num(action_probs, nan=1e-6)\n",
        "        action_probs = torch.clamp(action_probs, min=1e-6)\n",
        "        # print(action_probs)\n",
        "        # 构造 Dirichlet 分布，注意这里使用一个浓度参数乘以概率向量\n",
        "        concentration = action_probs * self.dirichlet_concentration\n",
        "        dist = torch.distributions.Dirichlet(concentration)\n",
        "        action = dist.rsample()  # 重参数化采样\n",
        "        return action, dist\n",
        "\n",
        "    def get_value(self, simulation):\n",
        "        \"\"\"\n",
        "        仅返回当前状态的价值评估（critic 部分）。\n",
        "        \"\"\"\n",
        "        env_vector, X_invariant, X_variant, A = self.get_state(simulation)\n",
        "        _, value = self.network(env_vector, X_invariant, X_variant, A)\n",
        "        return value\n",
        "\n",
        "    def update_policy(self, transitions):\n",
        "        \"\"\"\n",
        "        根据收集到的 transitions 来更新策略。\n",
        "        transitions 是一个列表，每个元素为 (env_vector, X_invariant, X_variant, A, action, reward, next_env_vector, next_X_invariant, next_X_variant, next_A, done)\n",
        "        其中 done 为布尔值，表示是否终止。\n",
        "\n",
        "        使用 A2C 算法（单步 TD 估计）：\n",
        "            advantage = r + gamma * V(next) * (1 - done) - V(current)\n",
        "            Actor loss: -log(prob(a)) * advantage (加上 entropy bonus)\n",
        "            Critic loss: MSE(V(current), r + gamma*V(next) * (1-done))\n",
        "        \"\"\"\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        entropy_losses = []\n",
        "\n",
        "        for transition in transitions:\n",
        "            (env_vector, X_invariant, X_variant, A, action_taken, old_log_prob, reward,\n",
        "             next_env_vector, next_X_invariant, next_X_variant, next_A, done) = transition\n",
        "            # 当前状态的网络输出\n",
        "            probs, value = self.network(env_vector, X_invariant, X_variant, A)\n",
        "\n",
        "            # print(probs)\n",
        "            concentration = torch.clamp(probs, min=1e-6) * self.dirichlet_concentration\n",
        "            dist = torch.distributions.Dirichlet(concentration)\n",
        "            # 计算对数概率，action_taken 是一个连续向量（tensor）\n",
        "            log_prob = dist.log_prob(action_taken)\n",
        "\n",
        "            # print(f\"action: {action_taken}, log_prob: {log_prob}\")\n",
        "            # log_prob = dist.log_prob(torch.tensor(action_taken))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # 计算下一状态价值（如果 done 则值为0）\n",
        "            with torch.no_grad():\n",
        "                _, next_value = self.network(next_env_vector, next_X_invariant, next_X_variant, next_A)\n",
        "                target = reward + self.gamma * next_value * (1 - done)\n",
        "            # print(f\"target: {target}, value {value}\")\n",
        "            # 计算优势函数\n",
        "            advantage = target - value #正2.5\n",
        "\n",
        "             # PPO 比率\n",
        "            ratio = torch.exp(log_prob - old_log_prob)\n",
        "            # Clipped ratio\n",
        "            epsilon = 1e-4\n",
        "            clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "            # PPO actor loss（取两者中较小者）\n",
        "            actor_loss = -torch.min(ratio * advantage.detach(), clipped_ratio * advantage.detach())\n",
        "            # Critic loss（Smooth L1 Loss）\n",
        "            critic_loss = F.smooth_l1_loss(value, target.detach())\n",
        "\n",
        "            # Entropy bonus（鼓励探索）\n",
        "            # print(f\"original advantage: {advantage}\")\n",
        "            # Actor loss\n",
        "            # print(log_prob)\n",
        "            # actor_loss = -log_prob * advantage.detach()\n",
        "            # Critic loss（均方误差）\n",
        "            critic_loss = F.smooth_l1_loss(value, target.detach())\n",
        "            # print(f\"actor_loss: {actor_loss}\")\n",
        "            # print(f\"critic_loss: {critic_loss}\")\n",
        "            # Entropy bonus（鼓励探索）\n",
        "            entropy = dist.entropy()\n",
        "            # print(f\"entropy: {entropy}\")\n",
        "            actor_losses.append(actor_loss - entropy*0.1)\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "        # print(actor_losses)\n",
        "        loss = torch.stack(actor_losses).mean() + torch.stack(critic_losses).mean()\n",
        "\n",
        "        # print(f\"loss: {loss}\")\n",
        "        self.loss_history.append(loss.item())\n",
        "        check_parameters(self.network.actor_mlp)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.network.actor_mlp.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        check_parameters(self.network.gcn_fc)\n",
        "\n",
        "\n",
        "def train_agent(num_regions, num_episodes=1, max_steps=5):\n",
        "    # 初始化 simulation 和 agent\n",
        "    sim = WildfireSimulation(num_regions)\n",
        "    agent = Agent(num_nodes=sim.num_regions, env_dim=5, hidden_dim=64, lr=0.1)\n",
        "    # 存储所有 episode 的奖励\n",
        "    episode_rewards = []\n",
        "    final_rewards = [0.1,0.5,0.7,0.9,1.0,0.1,0.5,0.01,0.05]\n",
        "    step_rewards = [-0.1, -0.2, -0.1, -0.4, -0.5,-0.7,-0.1,-0.2,-0.2,-0.3,-0.4,-0.02]\n",
        "    setps_to_contaiminated = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        if episode % 100 == 0:\n",
        "          print(f\"===== Episode {episode} =====\")\n",
        "        # 重置 simulation：重新初始化节点状态和历史记录\n",
        "        sim.__init__(num_regions=sim.num_regions)  # 重新构造环境\n",
        "        sim.history = []  # 清空历史记录\n",
        "        # 在这里可选：预先记录初始状态\n",
        "        # 记录 transitions 用于更新策略\n",
        "        transitions = []\n",
        "        total_reward = 0.0\n",
        "        t = 0\n",
        "        # 运行一个 episode（用 run_session_TD 或逐步调用 next()）\n",
        "        # 这里我们使用逐步推进的方式：\n",
        "        # 先记录初始状态\n",
        "        initial_env = {\n",
        "            'wind_direction': None,\n",
        "            'wind_strength': None,\n",
        "            'dryness': None,\n",
        "            'temperature': None\n",
        "        }\n",
        "        state = {node_id: {'pos': sim.nodes[node_id].pos.copy(), 'H': sim.nodes[node_id].H, 'K': sim.nodes[node_id].K}\n",
        "                 for node_id in sim.nodes}\n",
        "        sim.history.append((0, initial_env, state, None))\n",
        "        done = False\n",
        "        while t < max_steps and not done:\n",
        "            t += 1\n",
        "            # 执行一步模拟（内部会记录状态及 agent action）\n",
        "            sim.next(t, agent)\n",
        "            # 获取当前状态（s)和下一状态 (s') 的相关图信息，构造 transition\n",
        "            # 这里简单地认为当前状态为 sim.history[-2]，下一状态为 sim.history[-1]\n",
        "            s_record = sim.history[-2]\n",
        "            s_env, s_state, _ = s_record[1], s_record[2], s_record[3]\n",
        "            ns_record = sim.history[-1]\n",
        "            ns_env, ns_state, _ = ns_record[1], ns_record[2], ns_record[3]\n",
        "            # 构造 env_vector, X_invariant, X_variant, A\n",
        "            env_vector, X_invariant, X_variant, A = agent.get_state(sim)\n",
        "            # 对于下一状态，也重新构造（这里简单调用 get_state 后略作修改）\n",
        "            # 注意：为了简单，假设 next_state 的输入与当前状态类似（实际应由 ns_record 构造）\n",
        "            next_env_vector, next_X_invariant, next_X_variant, next_A = agent.get_state(sim)\n",
        "            # 获得 agent 选择的 action（保存为一个离散的 index，假设取概率最大的 index）\n",
        "            # action_probs = agent.action(sim)\n",
        "            # action_index = int(np.argmax(action_probs))\n",
        "            action, dist = agent.action(sim)\n",
        "            old_log_prob = dist.log_prob(action)\n",
        "\n",
        "\n",
        "            # 计算奖励：根据文档：R = -1000*(H_total_{t+1} - H_total_t)/(max_health_total) - 10\n",
        "            H_total_t = np.sum([s_state[node]['H'] for node in s_state])\n",
        "            H_total_next = np.sum([ns_state[node]['H'] for node in ns_state])\n",
        "            max_health_total = np.sum([sim.nodes[node].max_health for node in sim.nodes])\n",
        "            r = -(H_total_next - H_total_t)/max_health_total - 0.01\n",
        "            r = (r - np.mean(step_rewards)) / (np.std(step_rewards) + 1e-4)\n",
        "            step_rewards.append(r)\n",
        "\n",
        "            # 如果模拟结束，则给予终止奖励 1000\n",
        "            if sim.is_end():\n",
        "                setps_to_contaiminated.append(t)\n",
        "                if episode % 100 == 0:\n",
        "                  print(f\"game end at {t} session, and get final reward\")\n",
        "                  sim.plot_at_t(t)\n",
        "                Ht_values = [s_state[node]['H'] for node in s_state]\n",
        "\n",
        "                num_protected = np.sum([1 for node in s_state if (sim.nodes[node].max_health-s_state[node]['H'])/sim.nodes[node].max_health <= 0.10 ])\n",
        "\n",
        "                # r = np.sum(Ht_values) - max_health_total + num_protected*700\n",
        "                r = num_protected - sim.num_regions/2.0\n",
        "\n",
        "                mean_reward = np.mean(final_rewards)\n",
        "                std_reward = np.std(final_rewards) + 1e-8\n",
        "                normalized_r = (r - mean_reward) / std_reward\n",
        "                final_rewards.append(normalized_r) #为了做reward归一化\n",
        "                r = normalized_r\n",
        "\n",
        "                done = True\n",
        "            total_reward += r\n",
        "            transition = (env_vector, X_invariant, X_variant, A,\n",
        "                          action, old_log_prob, torch.tensor(r, dtype=torch.float),\n",
        "                          next_env_vector, next_X_invariant, next_X_variant, next_A,\n",
        "                          torch.tensor(float(done)))\n",
        "            transitions.append(transition)\n",
        "        # 更新 agent 策略（一次 episode 内采样的 transitions，用 A2C 单步 TD 更新）\n",
        "        agent.update_policy(transitions)\n",
        "        episode_rewards.append(total_reward)\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode} total reward: {total_reward}\")\n",
        "            print(f\"Episode {episode} agent prev loss: {agent.loss_history[-1]}\")\n",
        "\n",
        "    return agent, episode_rewards, agent.loss_history, setps_to_contaiminated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_global_random_seed(seed):\n",
        "    random.seed(seed)  # Python random module\n",
        "    np.random.seed(seed)  # NumPy random module\n",
        "    torch.manual_seed(seed)  # PyTorch random module\n",
        "    torch.cuda.manual_seed(seed)  # For CUDA-specific operations\n",
        "    torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
        "    torch.backends.cudnn.deterministic = True  # Make CuDNN deterministic\n",
        "    torch.backends.cudnn.benchmark = False  # Disable CuDNN auto-tuning for reproducibility\n",
        "\n",
        "\n",
        "set_global_random_seed(123)\n",
        "\n",
        "trained_agent, rewards, losses, steps_to_peace = train_agent(20, num_episodes=1000, max_steps=100)\n"
      ],
      "metadata": {
        "id": "iyxnBi6XZ3Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "window = 200\n",
        "smooth = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
        "plt.plot(smooth)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Training loss Curve\")\n",
        "plt.show()\n",
        "\n",
        "window = 600\n",
        "smooth_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "plt.plot(smooth_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"total rewards each ep\")\n",
        "plt.title(\"total rewards each Curve\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "window = 600\n",
        "smooth_steps_to_peace = np.convolve(steps_to_peace, np.ones(window)/window, mode='valid')\n",
        "plt.plot(smooth_steps_to_peace)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"total steps_to_peace each ep\")\n",
        "plt.title(\"total steps_to_peace each Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOCfDXupZ3Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_length = 5\n",
        "\n",
        "smooth_steps_to_peace = [\n",
        "    np.mean(steps_to_peace[i:i + window])\n",
        "    for i in range(0, len(steps_to_peace) - window + 1, step_length)\n",
        "]\n",
        "\n",
        "# Create x-axis values for plotting\n",
        "x_values = range(0, len(smooth_steps_to_peace) * step_length, step_length)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(x_values, smooth_steps_to_peace, label=f\"Moving Avg (Window={window}, Step={step_length})\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total steps_to_peace each ep\")\n",
        "plt.title(\"Total steps_to_peace each Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AeGGLlMCZ3Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moXaBfloZ3aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EnFLKJgKZ3cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xABblUCZ3ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCrpJ_WdZ3gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5BH-OEG9Z3iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ym_Jlt_wZ3kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack([torch.Tensor([60.00]),torch.Tensor([900.00])]).mean()"
      ],
      "metadata": {
        "id": "LAoKA5WmZ3mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7a9fk47Ik4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}